{
  "evaluation_criteria": [
    {
      "category": "accuracy",
      "weight": 0.4,
      "description": "Evaluates the factual correctness and precision of the model's responses",
      "questions": [
        "How accurate is the model's response to the given prompt?",
        "Does the response contain any factual errors or misconceptions?",
        "Are the claims made in the response well-supported and verifiable?",
        "Does the model demonstrate correct understanding of the topic?"
      ]
    },
    {
      "category": "coherence",
      "weight": 0.3,
      "description": "Assesses the logical structure and flow of the model's responses",
      "questions": [
        "Is the response logically structured and well-organized?",
        "Does the response flow naturally from one point to the next?",
        "Are the ideas presented in a clear and coherent manner?",
        "Does the response maintain consistency throughout?"
      ]
    },
    {
      "category": "relevance",
      "weight": 0.3,
      "description": "Measures how well the response addresses the specific prompt or question",
      "questions": [
        "How relevant is the response to the original prompt?",
        "Does the response address all aspects of the question asked?",
        "Is the response focused and on-topic throughout?",
        "Does the model avoid unnecessary tangents or irrelevant information?"
      ]
    }
  ],
  "scoring_guidelines": {
    "excellent": {
      "range": "0.9-1.0",
      "description": "Outstanding performance with minimal to no issues"
    },
    "good": {
      "range": "0.7-0.89",
      "description": "Strong performance with minor issues"
    },
    "satisfactory": {
      "range": "0.5-0.69",
      "description": "Adequate performance with some notable issues"
    },
    "poor": {
      "range": "0.3-0.49",
      "description": "Below average performance with significant issues"
    },
    "very_poor": {
      "range": "0.0-0.29",
      "description": "Unacceptable performance with major issues"
    }
  },
  "evaluation_instructions": {
    "general": [
      "Evaluate each response objectively and consistently",
      "Consider the specific criteria for each category",
      "Provide scores as decimal values between 0.0 and 1.0",
      "Include brief reasoning for your scoring decisions",
      "Be fair and unbiased in your evaluations"
    ],
    "accuracy_specific": [
      "Verify factual claims where possible",
      "Consider the complexity and difficulty of the topic",
      "Penalize clear misinformation or false statements",
      "Reward precise and well-informed responses"
    ],
    "coherence_specific": [
      "Look for logical progression of ideas",
      "Assess clarity of expression and organization",
      "Consider readability and comprehension",
      "Evaluate consistency of tone and style"
    ],
    "relevance_specific": [
      "Ensure the response directly addresses the prompt",
      "Check for completeness in answering all parts",
      "Assess focus and avoidance of off-topic content",
      "Consider appropriateness of response length"
    ]
  },
  "metadata": {
    "version": "1.0",
    "created_date": "2024-01-01",
    "last_updated": "2024-01-01",
    "description": "Standard evaluation criteria for LLM leaderboard judging system"
  }
}